{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHB130NJvAIbqWiI07n5r7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvgenyEsin/Python_libraries_for_DS/blob/main/Python_libraries_for_DS_DZ_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ДЗ к семинару 9\n",
        "\n",
        "Цель задания: Исследовать влияние различных методов понижения размерности на качество классификации текстовых данных.\n",
        "\n",
        "Датасет: Набор данных новостных статей\n",
        "(датасет '20 Newsgroups' доступный в sklearn.datasets).\n",
        "\n",
        "Задачи:\n",
        "\n",
        "1. Загрузите датасет '20 Newsgroups' из sklearn.\n",
        "\n",
        "2. Проведите предобработку данных (очистка текста, удаление стоп-слов, векторизация с использованием TF-IDF).\n",
        "\n",
        "3. Примените к полученным векторам TF-IDF следующие методы понижения размерности:\n",
        "— PCA (Principal Component Analysis)\n",
        "— t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        "— UMAP (Uniform Manifold Approximation and Projection).\n",
        "\n",
        "4. После понижения размерности данных используйте любой метод машинного обучения для классификации новостей по темам.\n",
        "\n",
        "5. Сравните качество классификации для каждого метода понижения размерности. Используйте метрики точности и F1-меру.\n",
        "\n",
        "6. Визуализируйте двумерное представление данных для каждого метода понижения размерности, чтобы оценить, как алгоритмы справляются с сепарацией классов.\n",
        "\n",
        "7. Напишите отчёт, в котором обсудите, какой метод понижения размерности оказал наиболее значительное влияние на качество классификации и почему."
      ],
      "metadata": {
        "id": "vtoDeYg5n9Cz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KknogYicn34b",
        "outputId": "7120e904-ca9a-4254-9c55-c6b000478148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "!pip install umap-learn\n",
        "from umap import UMAP\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных\n",
        "\n",
        "мы будем использовать встроенный загрузчик наборов данных для выборки «The 20 newsgroups» из scikit-learn. Иначе, выборку можно загрузить вручную с вэб сайта, использовать функцию sklearn.datasets.load_files и указав папку «20news-bydate-train» для сохранения распакованного архива.\n",
        "Чтобы первый пример быстрее исполнялся, мы будем работать только с частью нашего набора данных, разбитой на 4 категории из 20 возможных:"
      ],
      "metadata": {
        "id": "z7bdqoeKwGyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False, n_retries=3, delay=1.0)"
      ],
      "metadata": {
        "id": "ZV0BvATCrn_K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы можем загрузить список файлов, совпадающих с нужными категориями, как показано ниже:"
      ],
      "metadata": {
        "id": "5IyCp9zt3HJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возвращаемый набор данных — это scikit-learn совокупность: одномерный контейнер с полями, которые могут интерпретироваться как ключи в словаре python (dict keys), проще говоря — как признаки объекта (object attributes). Например, target_names содержит список названий запрошенных категорий:"
      ],
      "metadata": {
        "id": "CpLoQ0ts3hfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "cats = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42)\n",
        "list(newsgroups_train.target_names)"
      ],
      "metadata": {
        "id": "e1MVdPgXroBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36fec2d-6e0f-47c5-dfd0-286db0b0a5f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сами файлы загружаются в память как атрибут data. Вы также можете ссылаться на названия файлов:"
      ],
      "metadata": {
        "id": "OVVMCrwY3vV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.filenames.shape"
      ],
      "metadata": {
        "id": "7U9pGQV_roEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e595b765-4290-4495-b452-d9f114cd86b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij3BCIcp1ibg",
        "outputId": "2d07746f-1837-4fdf-e341-2cc95cfa83e4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257,)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте выведем на печать первые строки из первого загруженного файла:"
      ],
      "metadata": {
        "id": "KqIeuXK23z-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(newsgroups_train.data[0].split(\"\\n\")[:3]))\n",
        "\n",
        "print(newsgroups_train.target_names[newsgroups_train.target[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rT60Go01ij3",
        "outputId": "37389644-729e-4c5a-8a7e-909d4ad49a5e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n",
            "comp.graphics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритмы для обучения с учителем (контролируемого обучения) требуют, чтобы у каждого документа в обучающей выборке была помета определенной категории. В нашем случае, категория — это название новостной выборки, которая «случайно» оказывается названием папки, содержащей характерные документы.\n",
        "Для увеличения скорости и эффективного использования памяти, scikit-learn загружает целевой атрибут как массив целых чисел, который соответствует индексу названия категории из списка target_names. Индекс категории каждой выборки хранится в атрибуте target:"
      ],
      "metadata": {
        "id": "BQtVh-kG4gn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.target[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVGRGKIK1imr",
        "outputId": "5850f4ab-7433-4f02-d8ba-50bc45cdd971"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечение характерных признаков из текстовых файлов\n",
        "\n",
        "Чтобы использовать машинное обучение на текстовых документах, первым делом, нужно перевести текстовое содержимое в числовой вектор признаков.\n",
        "«Мешок слов» (набор слов)\n",
        "\n",
        "Наиболее интуитивно понятный способ сделать описанное выше преобразование — это представить текст в виде набора слов:\n",
        "приписать уникальный целочисленный индекс каждому слову, появляющемуся в документах в обучающей выборке (например, построив словарь из слов с целочисленными индексами).\n",
        "для каждого документа #i посчитать количество употреблений каждого слова w и сохранить его (количество) в X[i, j]. Это будет значение признака #j, где j — это индекс слова w в словаре.\n",
        "\n",
        "Представление «мешок слов» подразумевает, что n_features — это некоторое количество уникальных слов в корпусе. Обычно, это количество превышает 100000.\n",
        "Если n_samples == 10000, то Х, сохраненный как массив numpy типа float32, потребовал бы 10000 x 100000 x 4 bytes = 4GB оперативной памяти (RAM), что едва осуществимо в современным компьютерах.\n",
        "К счастью, большинство значений в X являются нулями, поскольку в одном документе используется менее чем пара сотен уникальных слов. Поэтому «мешок слов» чаще всего является высоко размерным разреженным набором данных. Мы можем сэкономить много свободной оперативки, храня в памяти только лишь ненулевые части векторов признаков.\n",
        "Матрицы scipy.sparse — это структуры данных, которые именно это и делают — структурируют данные. В scikit-learn есть встроенная поддержка этих структур.\n",
        "\n",
        "Токенизация текста с scikit-learn\n",
        "\n",
        "Предобработка текста, токенизация и отфильтровывание стоп-слов включены в состав высоко уровневого компонента, который позволяет создать словарь характерных признаков и перевести документы в векторы признаков:"
      ],
      "metadata": {
        "id": "IWpF-5-J42wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(newsgroups_train.data)\n",
        "X_train_counts.shape"
      ],
      "metadata": {
        "id": "EdVbSDg0roHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512f7ebd-44e9-438f-da30-3660902c1066"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer поддерживает подсчет N-грам слов или последовательностей символов. Векторизатор строит словарь индексов признаков:"
      ],
      "metadata": {
        "id": "9sqWxGCS5N6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.vocabulary_.get(u'algorithm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP25J1ft4rk9",
        "outputId": "523e4b54-8327-452d-da91-e2892ace213a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Значение индекса слова в словаре связано с его частотой употребления во всем обучающем корпусе.\n",
        "\n",
        "От употреблений к частотности\n",
        "\n",
        "Подсчет словоупотреблений — это хорошее начало, но есть проблема: в длинных документах среднее количество словоупотреблений будет выше, чем в коротких, даже если они посвящены одной теме.\n",
        "Чтобы избежать этих потенциальных несоответствий, достаточно разделить количество употреблений каждого слова в документе на общее количество слов в документе. Этот новый признак называется tf — Частота термина.\n",
        "Следующее уточнение меры tf — это снижение веса слова, которое появляется во многих документах в корпусе, и отсюда является менее информативным, чем те, которые используются только в небольшой части корпуса. Примером низко ифнормативных слов могут служить служебные слова, артикли, предлоги, союзы и т.п.\n",
        "Это снижение называется tf–idf, что значит “Term Frequency times Inverse Document Frequency” (обратная частота термина).\n",
        "Обе меры tf и tf–idf могут быть вычислены следующим образом:"
      ],
      "metadata": {
        "id": "619m-7Kr5WQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
        "X_train_tf = tf_transformer.transform(X_train_counts)\n",
        "X_train_tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWVTybMO4rn_",
        "outputId": "7d7cfcd1-1799-472e-e767-0934c3b5c898"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В примере кода, представленного выше, мы сначала используем метод fit(..), чтобы прогнать наш алгоритм оценки на данных, а потом — метод transform(..), чтобы преобразовать нашу числовую матрицу к представлению tf-idf. Эти два шага могут быть объединены и дадут тот же результат на выходе, но быстрее, что можно сделать с помощью пропуска излишней обработки. Для этого нужно использовать метод fit_transform(..), как показано ниже:"
      ],
      "metadata": {
        "id": "mW1r23p65myY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaqLtsY-4rqd",
        "outputId": "ea6024cb-26ec-4819-f1f3-007c9675442e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение классификатора\n"
      ],
      "metadata": {
        "id": "jwNV5Po-66Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда мы выделили признаки, можно обучать классификатор предсказывать категорию текста. Давайте начнем с Наивного Байесовского классификатора, который станет прекрасной отправной точкой для нашей задачи. scikit-learn включает в себя несколько вариантов этого классификатора. Самый подходящий для подсчета слов — это его поли номинальный вариант:"
      ],
      "metadata": {
        "id": "6EMdxJz866lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, newsgroups_train.target)"
      ],
      "metadata": {
        "id": "fE7vTxZ-4rtJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы мы могли попытаться предсказать результаты на новом документе, нужно извлечь фичи (характерные признаки), используя почти такую же последовательность как и ранее. Разница состоит в том, используется метод transform вместо fit_transform из transformers, так как они уже были применены к нашей обучающей выборке:"
      ],
      "metadata": {
        "id": "bI6caL8u7Q7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "predicted = clf.predict(X_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "  print('%r => %s' % (doc, newsgroups_train.target_names[category]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prrIOwEt4rvo",
        "outputId": "b14365ea-1350-495b-feae-32ff411a9c0b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'God is love' => soc.religion.christian\n",
            "'OpenGL on the GPU is fast' => comp.graphics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создание конвейерной обработки"
      ],
      "metadata": {
        "id": "LamgwGDp7ku9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы с цепочкой vectorizer => transformer => classifier было проще работать, в scikit-learn есть класс Pipeline, который функционирует как составной (конвейерный) классификатор:"
      ],
      "metadata": {
        "id": "YIl2UQvv7mXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                    ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB()),\n",
        "                     ])"
      ],
      "metadata": {
        "id": "Qt393l3i4ryH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь обучим модель с помощью всего 1 команды:"
      ],
      "metadata": {
        "id": "nJmscX7C8QGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf = text_clf.fit(newsgroups_train.data, newsgroups_train.target)"
      ],
      "metadata": {
        "id": "JAIqC85d4r0p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оценка производительности при работе на тестовой выборке\n",
        "\n",
        "Оценка точности прогноза модели достаточно проста:"
      ],
      "metadata": {
        "id": "vYki2izt8Zad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "    categories=cats, shuffle=True, random_state=42)\n",
        "docs_test = twenty_test.data\n",
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE_kxDV64r3Q",
        "outputId": "4233ca97-d9be-4e48-9254-b29f2c5dab21"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8348868175765646"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Например, мы получили 83% точности. Давайте посмотрим, можем ли мы улучшить этот результат с помощью линейного метода опорных векторов (support vector machine (SVM)). Этот метод обычно считается лучшим из алгоритмов классификации текста (хотя, он немного медленнее чем наивный Байес). Мы можем сменить обучающуюся модель просто подсоединив другой объект классификации в наш конвейер:"
      ],
      "metadata": {
        "id": "WzmRaiSo8s-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)),])\n",
        "_ = text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjdpBvdG4r5y",
        "outputId": "7cc4a90c-19d3-49d5-bec9-0c9f87e37589"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9114513981358189"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn также предоставляет утилиты для более детального анализа результатов:"
      ],
      "metadata": {
        "id": "ttrjlWg29R2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted,\n",
        "    target_names=twenty_test.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ekog09L8tvx",
        "outputId": "9e3a2f24-adf2-4f39-f0b2-0b46719a3fac"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.95      0.81      0.88       319\n",
            "         comp.graphics       0.87      0.98      0.92       389\n",
            "               sci.med       0.96      0.88      0.92       396\n",
            "soc.religion.christian       0.89      0.96      0.92       398\n",
            "\n",
            "              accuracy                           0.91      1502\n",
            "             macro avg       0.92      0.91      0.91      1502\n",
            "          weighted avg       0.92      0.91      0.91      1502\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.confusion_matrix(twenty_test.target, predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT-y8qEW8tyd",
        "outputId": "42b69d26-dfdf-4b86-a659-4445e32b9933"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[259,  10,  12,  38],\n",
              "       [  3, 380,   2,   4],\n",
              "       [  6,  36, 349,   5],\n",
              "       [  5,  10,   2, 381]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как и ожидалось, матрица неточностей показывает, что тексты из выборки newsgroups об атеизме и христианстве модель чаще путает друг с другом, нежели чем с текстами про компьютерную графику.\n",
        "\n",
        "Настройка параметров для использования grid search\n",
        "\n",
        "Некоторые параметры мы уже посчитали, такие как use_idf в функции TfidfTransformer. Классификаторы, как правило, также имеют много параметров, например, MultinomialNB включает в себя коэффициент сглаживания alpha, а SGDClassifier имеет штрафной параметр alpha (метод штрафных функций), настраиваемую потерю и штрафные члены в целевой функции (см. раздел документации или используйте функцию подсказки Python, чтобы получить дополнительную информацию).\n",
        "Вместо поиска параметров различных компонентов в цепи, можно запустить поиск (методом полного перебора ) лучших параметров в сетке возможных значений. Мы опробовали все классификаторы на словах или биграммах, с или без idf, с штрафными параметрами 0,01 и 0,001 для SVM (метода опорных векторов):"
      ],
      "metadata": {
        "id": "YYO7gh1P9n4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "              'tfidf__use_idf': (True, False),\n",
        "              'clf__alpha': (1e-2, 1e-3),\n",
        "}"
      ],
      "metadata": {
        "id": "-ObowxT_8t07"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очевидно, подобный поиск методом полного перебора может быть ресурсозатратным. Если в нашем распоряжении есть множество ядер процессора, мы можем запустить grid search, чтобы попробовать все комбинации параметров параллельно с параметром n_jobs. Если мы зададим этому параметру значение -1, grid search определит, как много ядер установлено, и задействует их все:"
      ],
      "metadata": {
        "id": "GnVOrxz2-fd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
      ],
      "metadata": {
        "id": "OunuXEjs8t3Z"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Экземпляр grid search ведет себя как обычная модель scikit-learn. Давайте запустим поиск на малой части обучающей выборки, чтобы увеличить скорость обработки:"
      ],
      "metadata": {
        "id": "Pufyj3gP-mBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_clf = gs_clf.fit(newsgroups_train.data[:400], newsgroups_train.target[:400])"
      ],
      "metadata": {
        "id": "3pEzzCGA8t53"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В результате применения метода fit на объекте GridSearchCV мы получим классификатор, который можно использовать для выполнения функции predict:"
      ],
      "metadata": {
        "id": "UlA0srVp-1AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.target[gs_clf.predict(['God is love'])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG0WImOx8t8Q",
        "outputId": "cb7d2e15-7108-4a82-c5bd-8af816b381fa"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8re6D_iE_1Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CaFP40Uz8uBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}